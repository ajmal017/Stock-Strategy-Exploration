---
output: html_document
editor_options: 
  chunk_output_type: console
---
---
title: "Stock Strategy Exploration and Automation"
author: 
- Abram Yorde
- Karen Richard
- Paul Fullenkamp
date: "October 1, 2018"
output: 
  html_document:
    theme: cerulean
    toc: true
    toc_depth: 1
    toc_float: true
---

```{r setup, include=FALSE}
######################## Functions ########################
is_installed = function(mypkg)
  is.element(mypkg, installed.packages()[, 1])
load_or_install <- function(package_names)
{
  for (package_name in package_names)
  {
    if (!is_installed(package_name))
    {
      install.packages(package_name, dependencies = TRUE)
    }
    library(
      package_name,
      character.only = TRUE,
      quietly = TRUE,
      verbose = FALSE
    )
  }
}
sourceDir <- function(path, trace = TRUE, ...) {
    for (nm in list.files(path, pattern = "\\.[RrSsQq]$")) {
       if(trace) cat(nm,":")           
       source(file.path(path, nm), ...)
       if(trace) cat("\n")
    }
}
##########################################################################
Required_Packages = c('tidyverse','installr','psych','quantmod','lubridate','dygraphs','doParallel','XML','earth', 'lubridate', 'googledrive','cumstats','dummy','caret','plotly')
load_or_install(Required_Packages)

## Checking for R updates
updateR(fast = F,
        browse_news = F,
        install_R = T,
        copy_packages = F,
        copy_Rprofile.site = T,
        keep_old_packages = F,
        update_packages = F,
        start_new_R = F,
        quit_R = T,
        print_R_versions = T,
        GUI = F,
        to_checkMD5sums = T,
        keep_install_file = F)

## Loading Required Functions
sourceDir(paste0(getwd(),'/Codes/Functions/'))

## Root Data Folder
Root_Folder = getwd()

## Disabling API Warning
options("getSymbols.yahoo.warning" = FALSE)
options("getSymbols.warning4.0" = FALSE)

## General RMD Options
Local_S = T
Local_L = F
Google_Drive_S = T
Google_Drive_L = T
Initial_Pull = T
Update_Pull = T

## Google Drive Maintinance
drive_empty_trash()
```

## Initial Raw Data Pull

```{r Initial Data Pull}
if(Initial_Pull){
  NASDAQ_Stocks = read.csv(paste0(Root_Folder,"/Data/NASDAQ.csv"))
  AMEX_Stocks = read.csv(paste0(Root_Folder,"/Data/AMEX.csv"))
  NYSE_Stocks = read.csv(paste0(Root_Folder,"/Data/NYSE.csv"))
  Market_Tickers = data.frame(Symbol = c("^GSPC","^IXIC","^DJI"),
                              Name = c("S&P 500","NASDAQ","Dow Jones"),
                              IPOyear = as.character(year(Sys.Date()) - 5),
                              LastSale = "1000")
  
  Total_Stocks = bind_rows(Market_Tickers,NASDAQ_Stocks) %>%
    mutate(IPOyear = as.numeric(as.character(IPOyear)),
           LastSale = as.numeric(as.character(LastSale))) %>%
    filter(!is.na(IPOyear),
           !is.na(LastSale)) %>%
    filter(IPOyear <= year(Sys.Date()) - 5,
           IPOyear >= year(Sys.Date()) - 15,
           LastSale >= 10)
  Dump = list()
  
  p = progress_estimated(n = nrow(Total_Stocks),min_time = 3)
  for(i in 1:nrow(Total_Stocks)){
    p$pause(0.1)$tick()$print()
    ticker = as.character(Total_Stocks$Symbol[i])
    
    
    stockData = try(getSymbols(
      ticker,
      src = "yahoo",
      auto.assign = FALSE) %>%
        as.data.frame() %>%
        mutate(Date = ymd(rownames(.))))
    if("try-error" %in% class(stockData)){
      Dump[[i]] = stockData
    }else{
      colnames(stockData) = c("Open","High","Low","Close","Volume","Adjusted","Date")
      stockData$Stock = ticker
      Dump[[i]] = stockData
    }
  }
  list.condition <- sapply(Dump, function(x) class(x) == "data.frame")
  output.list  <- Dump[list.condition]
  Combined_Results = plyr::ldply(output.list,data.frame)
  
  Combined_Results = Combined_Results %>%
    group_by(Stock) %>%
    na.locf() %>%
    ungroup()
  
  
  if(Local_S){
    save(Combined_Results,
         file = paste0(Root_Folder,"/Data//NASDAQ Historical.RDATA"))
  }
  if(Google_Drive_S){
    save(Combined_Results,
         file = paste0(tempdir(),'/',"NASDAQ Historical.RDATA"))
    Check = drive_find("NASDAQ Historical.RDATA")
    if(nrow(Check) > 0){
      drive_rm("NASDAQ Historical.RDATA")
    }
    drive_upload(media =paste0(tempdir(),'/',"NASDAQ Historical.RDATA"),
                   path = "NASDAQ Historical.RDATA")
  }
}

```

```{r Data Update}
# Takes About 15 Minutes
if(Update_Pull){
  if(Local_L){
    Ticker_Pull_Function(Location = paste0(Root_Folder,"/Data/"),Google_Drive = F)
  }
  if(Google_Drive_L){
    Ticker_Pull_Function(Google_Drive = T)
  }
}
```

```{r Market Designation}
if(Google_Drive_L){
  File = drive_download(file = "NASDAQ Historical.RDATA",
                        overwrite = T)
  load(File$local_path)
  rm(File)
}
if(Local_L){
  load(paste0(Root_Folder,"/Data/NASDAQ Historical.RDATA"))
}

## Defining Market Directions
Market_DF = Combined_Results %>%
  filter(Stock %in% c("^GSPC","^IXIC","^DJI")) %>%
  group_by(Stock) %>%
 mutate(Indicator = runMax(Adjusted,90),
        Delta = (Adjusted -Indicator)/Indicator) %>%
  na.omit() %>%
  mutate(Market_Status = as.factor(case_when(
    Delta <= -0.20 ~ "Bear",
    Delta <= -0.10 ~ "Correction",
    Delta < -0.05 ~ "Pullback",
    Delta >= -0.05 ~ "Bull"
  ))) %>%
  ungroup()

Plot_DF = Market_DF
Plot_DF[Plot_DF$Stock == "^GSPC","Stock"] = "S&P 500"
Plot_DF[Plot_DF$Stock == "^IXIC","Stock"] = "NASDAQ"
Plot_DF[Plot_DF$Stock == "^DJI","Stock"] = "Dow Jones"
Plot_DF$Market_Status = factor(Plot_DF$Market_Status, levels = c("Bull",
                                                                 "Pullback",
                                                                 "Correction",
                                                                 "Bear"))

## Plot Examining Market Direction Designation
p1 = ggplot(Plot_DF,aes(x = Date,y = Adjusted,color = Market_Status)) +
  geom_point() +
  labs(x = "Date",
       y = "Adjusted",
       color = "Market Status") +
  facet_wrap(Stock~.,nrow = 3,ncol = 1,scales = "free_y")
print(p1)

## Final_Indicator
Market_Ind = Market_DF %>%
  select(Date,Stock,Market_Status) %>%
  spread(key = Stock, value = Market_Status) %>%
  mutate(Market_Status = case_when(
    `^GSPC` == "Bear" | `^IXIC` == "Bear" | `^DJI` == "Bear" ~ "Bear",
    `^GSPC` == "Correction" | `^IXIC` == "Correction" | `^DJI` == "Correction" ~ "Correction",
    `^GSPC` == "Pullback" | `^IXIC` == "Pullback" | `^DJI` == "Pullback" ~ "Pullback",
    TRUE ~ "Bull"
  )) %>%
  select(Date,Market_Status)

if(Local_S){
save(Market_Ind,
     file = paste0(Root_Folder,"/Data//Market_Ind.RDATA"))
}
if(Google_Drive_S){
    save(Market_Ind,
         file = paste0(tempdir(),'/',"Market_Ind.RDATA"))
    Check = drive_find("Market_Ind.RDATA")
    if(nrow(Check) > 0){
      drive_rm("Market_Ind.RDATA")
    }
    drive_upload(media =paste0(tempdir(),'/',"Market_Ind.RDATA"),
                   path = "Market_Ind.RDATA")
}
```


```{r Pool Reduction}
if(Google_Drive_L){
  File = drive_download(file = "NASDAQ Historical.RDATA",
                        overwrite = T)
  load(File$local_path)
  rm(File)
  File = drive_download(file = "Market_Ind.RDATA",
                        overwrite = T)
  load(File$local_path)
  rm(File)
}
if(Local_L){
  load(paste0(Root_Folder,"/Data/NASDAQ Historical.RDATA"))
  load(paste0(Root_Folder,"/Data//Market_Ind.RDATA"))
}

## Takes about 5 Minutes  
Start = Sys.time()
print("Initial Stat Calculation for Pool Selection")
PR_Stage = PR_Appendage(Combined_Results,parallel = T)
Sys.time() - Start

## Initial Pool Reduction
NASDAQ_Stocks = read.csv(paste0(Root_Folder,"/Data/NASDAQ.csv"))
PR_Red_1 = left_join(PR_Stage,NASDAQ_Stocks,by = c("Stock" = "Symbol")) %>%
  select(-c(X,Summary.Quote,LastSale)) %>%
  mutate(Reward_Mean_Score = dense_rank(desc(Reward_Mean)),
         Risk_Mean_Score = dense_rank(desc(Risk_Mean)),
         Price_Growth_Score = dense_rank(desc(Price_Growth)),
         Volume_Norm_Score = dense_rank(desc(Volume_Norm))) %>%
  na.omit()

## Appending FinViz Stats
## Takes About 2 Minutes
Start = Sys.time()
storage = list()
print("Beginning Pool Selection FinViz Stat Pull")
p = progress_estimated(nrow(PR_Red_1))
for(i in 1:nrow(PR_Red_1)){
  Ticker = PR_Red_1$Stock[i]
  TMP = try(FinViz_Metric_Pull(Ticker),
            silent = T)
  storage[[i]] = TMP
  p$pause(0.5)$tick()$print()
}
Metrics = plyr::ldply(storage[sapply(storage,class) %in% "data.frame"],data.frame)
Sys.time() - Start

Pool_Results = PR_Red_1 %>%
  left_join(Metrics) %>%
  mutate(Profit.Margin = as.numeric(str_remove(Profit.Margin,"%"))/100,
         Oper..Margin = as.numeric(str_remove(Oper..Margin,"%"))/100,
         Change = as.numeric(str_remove(Change,"%"))/100,
         Price = as.numeric(as.character(Price)),
         Prev.Close = as.numeric(as.character(Prev.Close)),
         ATR = as.numeric(as.character(ATR)),
         Beta = as.numeric(as.character(Beta)),
         Perf.YTD = as.numeric(str_remove(Perf.YTD,"%"))/100,
         Perf.Year = as.numeric(str_remove(Perf.Year,"%"))/100,
         Perf.Half.Y = as.numeric(str_remove(Perf.Half.Y,"%"))/100,
         Perf.Quarter = as.numeric(str_remove(Perf.Quarter,"%"))/100,
         Perf.Month = as.numeric(str_remove(Perf.Month,"%"))/100,
         Perf.Week = as.numeric(str_remove(Perf.Week,"%"))/100,
         RSI..14. = as.numeric(as.character(RSI..14.)),
         EPS..ttm. = as.numeric(str_remove(EPS..ttm.,"%"))/100,
         EPS.next.Y = as.numeric(str_remove(EPS.next.Y,"%"))/100,
         EPS.next.Q = as.numeric(str_remove(EPS.next.Q,"%"))/100,
         EPS.this.Y = as.numeric(str_remove(EPS.this.Y,"%"))/100,
         EPS.next.Y.1 = as.numeric(str_remove(EPS.next.Y.1,"%"))/100,
         EPS.next.5Y = as.numeric(str_remove(EPS.next.5Y,"%"))/100,
         EPS.past.5Y = as.numeric(str_remove(EPS.past.5Y,"%"))/100,
         Sales.past.5Y = as.numeric(str_remove(Sales.past.5Y,"%"))/100,
         Sales.Q.Q = as.numeric(str_remove(Sales.Q.Q,"%"))/100,
         EPS.Q.Q = as.numeric(str_remove(EPS.Q.Q,"%"))/100,
         ROA = as.numeric(str_remove(ROA,"%"))/100,
         ROE = as.numeric(str_remove(ROE,"%"))/100,
         ROI = as.numeric(str_remove(ROI,"%"))/100,
         X52W.High = as.numeric(str_remove(X52W.High,"%"))/100,
         X52W.Low = as.numeric(str_remove(X52W.Low,"%"))/100,
         Target.Price = as.numeric(as.character(Target.Price)),
         Short.Ratio = as.numeric(as.character(Short.Ratio)),
         Short.Float = as.numeric(str_remove(Short.Float,"%"))/100,
         Payout = as.numeric(str_remove(Payout,"%"))/100,
         Gross.Margin = as.numeric(str_remove(Gross.Margin,"%"))/100,
         Inst.Trans = as.numeric(str_remove(Inst.Trans,"%"))/100,
         Inst.Own = as.numeric(str_remove(Inst.Own,"%"))/100,
         Insider.Trans = as.numeric(str_remove(Insider.Trans,"%"))/100,
         Insider.Own = as.numeric(str_remove(Insider.Own,"%"))/100,
         X52W.Low = as.numeric(str_remove(X52W.Low,"%"))/100,
         LT.Debt.Eq = as.numeric(as.character(LT.Debt.Eq)),
         Debt.Eq = as.numeric(as.character(Debt.Eq)),
         Current.Ratio = as.numeric(as.character(Current.Ratio)),
         Quick.Ratio = as.numeric(as.character(Quick.Ratio)),
         P.FCF = as.numeric(as.character(P.FCF)),
         P.C = as.numeric(as.character(P.C)),
         P.B = as.numeric(as.character(P.B)),
         P.S = as.numeric(as.character(P.S)),
         PEG = as.numeric(as.character(PEG)),
         Forward.P.E = as.numeric(as.character(Forward.P.E)),
         P.E = as.numeric(as.character(P.E)),
         Recom = as.numeric(as.character(Recom)),
         Employees = as.numeric(as.character(Employees)),
         Outlook.Growth = (Target.Price - Prev.Close)/Prev.Close
         ) %>%
  filter(Outlook.Growth > 0,
         ROE >= 0.10,
         ROA > 0,
         ROI > 0,
         Insider.Own > 0,
         Sales.Q.Q > 0,
         EPS.Q.Q > 0,
         EPS.next.Y.1 > 0) %>%
  select(-c(Volume,Avg.Volume,Rel.Volume,SMA200,SMA50,SMA20)) 

Tickers = unique(Pool_Results$Stock)
for(i in 1:length(Tickers)){
  TMP = Combined_Results %>%
    filter(Stock == Tickers[i])
  p1 = ggplot(TMP,aes(x = Date,y = Adjusted)) + 
    geom_line() + 
    labs(title = paste(Tickers[i]))
  print(p1)
}

## Reducing Historical Data to Reduced Selections
Total_Results = Combined_Results %>%
  left_join(Market_Ind) %>%
  na.omit()

# Saving Pool Results and Reduced Raw Data
if(Local_S){
  save(Pool_Results,Total_Results,
       file = paste0(Root_Folder,"/Data//Pool_Results.RDATA"))
}
if(Google_Drive_S){
    save(Pool_Results,Total_Results,
         file = paste0(tempdir(),'/',"Pool_Results.RDATA"))
    Check = drive_find("Pool_Results.RDATA")
    if(nrow(Check) > 0){
      drive_rm("Pool_Results.RDATA")
    }
    drive_upload(media =paste0(tempdir(),'/',"Pool_Results.RDATA"),
                   path = "Pool_Results.RDATA")
}
```

## Training Set Creation

```{r Training Set Creation}
## Loading Pool Results
if(Google_Drive_L){
  File = drive_download("Pool_Results.RDATA",
                        overwrite = T)
  load(File$local_path)
  rm(File)
}
if(Local_L){
  load(file = paste0(Root_Folder,"/Data//Pool_Results.RDATA"))
}

## Building Optimized Training Set
## Takes About 3 Minute
Start = Sys.time()
Train_Set_Total = Training_Set_Function(Total_Results) %>%
  na.omit()
Sys.time() - Start

## Tabulating Optimized Window Sizes
Windows = Train_Set_Total %>%
  group_by(Stock) %>%
  select(contains("Window")) %>%
  summarise_all(mean) %>%
  ungroup() %>%
  select(-Stock) %>%
  summarise_all(median)


if(Local_S){
  save(Train_Set_Total,Windows,
       file = paste0(Root_Folder,"/Data/Training_Prep.RDATA"))
}
if(Google_Drive_S){
    save(Train_Set_Total,Windows,
         file = paste0(tempdir(),'/',"Training_Prep.RDATA"))
    Check = drive_find("Training_Prep.RDATA")
    if(nrow(Check) > 0){
      drive_rm("Training_Prep.RDATA")
    }
    drive_upload(media =paste0(tempdir(),'/',"Training_Prep.RDATA"),
                   path = "Training_Prep.RDATA")
}

```

```{r Total Train Exploration}
if(Google_Drive_L){
  File = drive_download("Training_Prep.RDATA",
                        overwrite = T)
  load(File$local_path)
  rm(File)
}
if(Local_L){
  load(file = paste0(Root_Folder,"/Data/Training_Prep.RDATA"))
}

## Removing Window Sizes and Expanding MArket Status Variable
Train_Set_Total_IMP = Train_Set_Total %>%
  select(-contains("Window")) %>%
  cbind(dummy(select(.,-c("Stock")),int = TRUE,verbose = T)) %>%
  select(-c(Market_Status))

## Checking Descriptive Stats on Technical Indicators
Sum_Stats = describe(x = Train_Set_Total_IMP,
                     na.rm = F,
                     ranges = T,
                     fast = T)

## Removing Indicators Which Are Unstable After Normalization
Keep = Sum_Stats %>%
  mutate(Vars = as.character(rownames(.))) %>%
  filter(!is.infinite(range)) %>%
  dplyr::select(Vars)
Keep = Keep$Vars
Train_Set_Total_IMP = Train_Set_Total_IMP %>%
  dplyr::select(Keep)

## Running Variable Importance On Total Training Set
Target = "Buy"
Remove = c("Date","Stock","Indicator","Max","W","Adjust_TMP")
Columns = Variable_Importance_Reduction(DF = Train_Set_Total_IMP,
                                        Target = Target,
                                        Remove = Remove)

## Saving Intermediate Results
if(Local_S){
  save(Columns,Train_Set_Total_IMP,
       file = paste0(Root_Folder,"/Data/Total_Set_Feat.RDATA"))
}
if(Google_Drive_S){
    save(Columns,Train_Set_Total_IMP,
         file = paste0(tempdir(),'/',"Total_Set_Feat.RDATA"))
    Check = drive_find("Total_Set_Feat.RDATA")
    if(nrow(Check) > 0){
      drive_rm("Total_Set_Feat.RDATA")
    }
    drive_upload(media =paste0(tempdir(),'/',"Total_Set_Feat.RDATA"),
                   path = "Total_Set_Feat.RDATA")
}
```

```{r Total Train Modeling}
if(Google_Drive_L){
  File = drive_download("Total_Set_Feat.RDATA",
                        overwrite = T)
  load(File$local_path)
  rm(File)
}
if(Local_L){
  load(file = paste0(Root_Folder,"/Data/Total_Set_Feat.RDATA"))
}
## Pulling Pool Selection Stocks
Stocks = unique(Train_Set_IMP$Stock)

## Creating Train / Test Sets
Train = Train_Set_Total_IMP %>%
  filter(!Stock %in% Stocks) %>%
  mutate(Buy = case_when(
           Indicator <= 0.65*Max ~ Buy,
           Indicator > 0.65*Max ~ 0
         ),
         Buy = as.factor(Buy)) %>%
  dplyr::select(Buy,Columns)
(table(Train$Buy))

Test = Train_Set_Total_IMP %>%
  filter(Stock %in% Stocks) %>%
   mutate(Buy = case_when(
           Indicator <= 0.65*Max ~ Buy,
           Indicator > 0.65*Max ~ 0
         ),
         Buy = as.factor(Buy)) %>%
  dplyr::select(Buy,Columns)
(table(Test$Buy))

# source("//climsidfs07/RefEng/1 Ref. Engineering (SH, Scroll & IPD)/13) Analytics/R Functions/Caret Model Compare.R")
# Model_Compare(Train = Train,
#               Test = Test,
#               Obs = "Buy",
#               Type = "C",
#               parallel = T,
#               p = 0.01,
#               Transform = F)

## Saving Intermediate Results
if(Local_S){
  save(Train,Test,
       file = paste0(Root_Folder,"/Data/Training_Data.RDATA"))
}
if(Google_Drive_S){
    save(Train,Test,
         file = paste0(tempdir(),'/',"Training_Data.RDATA"))
    Check = drive_find("Training_Data.RDATA")
    if(nrow(Check) > 0){
      drive_rm("Training_Data.RDATA")
    }
    drive_upload(media =paste0(tempdir(),'/',"Training_Data.RDATA"),
                   path = "Training_Data.RDATA")
}
```

## Top 10  Method Results
bagFDA - Bagged Flexible Discriminant Analysis
monmlp - Monotone Multi-Layer Perceptron NNet
avNNet - Model Averaged Neural Network
glm - Generalized Linear Model
multinom - Penalized Multinomial Regression
regLogistic - Regularized Logistic Regression 
bayesglm - Bayesian Generalized Linear Model
glmnet - glmnet
spls - Sparse Partial Least Squares
sparseLDA - Sparse Linear Discriminant Analysis

bagFDA is the least correlated, the rest are essentially one. Least correlated to bagFDA is the regularized logistic. Nice that they are also some of the simplest approaches as well.

```{r Modeling}
# Modeling with the crossfold function
if(Google_Drive_L){
  File = drive_download("Training_Data.RDATA",
                        overwrite = T)
  load(File$local_path)
  rm(File)
}
if(Local_L){
  load(file = paste0(Root_Folder,"/Data/Training_Data.RDATA"))
}

## Baseline no Pre Processing
mod_BSL = glm(Buy~.,
          data = Train,
          family = "binomial")
summary(mod_BSL)
Pred = predict(mod,Test,type = "response")
(AUC_BSL = MLmetrics::AUC(Pred,Test$Buy))
## Additional Variable Reduction
Imp = coef(summary(mod_BSL))[,4] %>%
  as.data.frame()
Imp$Var = rownames(Imp)
Imp = Imp %>%
  filter(`.` <= 0.001) %>%
  filter(Var != "(Intercept)")
Train = Train %>%
  select(Buy,Imp$Var)

## Redoing Model
mod_BSL = glm(Buy~.,
          data = Train,
          family = "binomial")
summary(mod_BSL)
Pred = predict(mod,Test,type = "response")
(AUC_BSL = MLmetrics::AUC(Pred,Test$Buy))

## Standard Pre Process
PP_Recipe = recipe(Buy~.,
                   data = Train) %>%
  step_interact(~.^2) %>%
  step_YeoJohnson(all_predictors() - contains("Market")) %>%
  step_center(all_predictors()- contians("Market")) %>%
  step_scale(all_predictors()) %>%
  step_pca(all_predictors(),
           threshold = 0.95)
PP_Prep = prep(PP_Recipe,Train)
Train.PP = bake(PP_Prep,Train)

## Round 1 Improvements
mod_R1 = glm(Buy~.,
          data = Train.PP,
          family = "binomial")
summary(mod_R1)
Pred = predict(mod,Test,type = "response")
(AUC_R1 = MLmetrics::AUC(Pred,Test$Buy))


```

