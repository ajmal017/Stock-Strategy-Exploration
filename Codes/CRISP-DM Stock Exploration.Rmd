---
output: html_document
editor_options: 
  chunk_output_type: console
---
---
title: "Stock Strategy Exploration and Automation"
author: 
- Abram Yorde
- Karen Richard
- Paul Fullenkamp
date: "October 1, 2018"
output: 
  html_document:
    theme: cerulean
    toc: true
    toc_depth: 1
    toc_float: true
---

```{r setup, include=FALSE}
######################## Functions ########################
is_installed = function(mypkg)
  is.element(mypkg, installed.packages()[, 1])
load_or_install <- function(package_names)
{
  for (package_name in package_names)
  {
    if (!is_installed(package_name))
    {
      install.packages(package_name, dependencies = TRUE)
    }
    library(
      package_name,
      character.only = TRUE,
      quietly = TRUE,
      verbose = FALSE
    )
  }
}
sourceDir <- function(path, trace = TRUE, ...) {
    for (nm in list.files(path, pattern = "\\.[RrSsQq]$")) {
       if(trace) cat(nm,":")           
       source(file.path(path, nm), ...)
       if(trace) cat("\n")
    }
}
##########################################################################
Required_Packages = c('tidyverse','installr','psych','quantmod','lubridate','dygraphs','doParallel','zoo','xts')
load_or_install(Required_Packages)

## Checking for R updates
updateR(fast = F,
        browse_news = F,
        install_R = T,
        copy_packages = F,
        copy_Rprofile.site = T,
        keep_old_packages = F,
        update_packages = F,
        start_new_R = F,
        quit_R = T,
        print_R_versions = T,
        GUI = F,
        to_checkMD5sums = T,
        keep_install_file = F)

Root_Folder = getwd()

## Loading Required Functions
sourceDir(paste0(getwd(),'/Codes/Functions/'))
```

## Initial Raw Data Pull

```{r Initial Data Pull}
# NASDAQ_Stocks = read.csv(paste0(Root_Folder,"/Data/NASDAQ.csv"))
# AMEX_Stocks = read.csv(paste0(Root_Folder,"/Data/AMEX.csv"))
# NYSE_Stocks = read.csv(paste0(Root_Folder,"/Data/NYSE.csv"))
# 
# Total_Stocks = bind_rows(NASDAQ_Stocks)
# Dump = list()
# 
# p = progress_estimated(n = nrow(Total_Stocks),min_time = 3)
# for(i in 1:nrow(Total_Stocks)){
#   p$pause(0.1)$tick()$print()
#   ticker = as.character(Total_Stocks$Symbol[i])
#   
# 
#   stockData = try(getSymbols(
#     ticker,
#     src = "yahoo",
#     auto.assign = FALSE) %>% 
#       as.data.frame() %>%
#       mutate(Date = ymd(rownames(.)))) 
#   if("try-error" %in% class(stockData)){
#     Dump[[i]] = stockData
#   }else{
#     colnames(stockData) = c("Open","High","Low","Close","Volume","Adjusted","Date")
#     stockData$Stock = ticker
#     Dump[[i]] = stockData
#   }
# }
# list.condition <- sapply(Dump, function(x) class(x) == "data.frame")
# output.list  <- Dump[list.condition]
# Combined_Results = plyr::ldply(output.list,data.frame)

# NA_Sum = function(x){Count = sum(is.na(x))}
# Check = Combined_Results %>%
#   group_by(Stock) %>%
#   summarise_all(NA_Sum)

# save(Combined_Results,
#      file = "//climsidfs07/RefEng/1 Ref. Engineering (SH, Scroll & IPD)/13) Analytics/Small Projects/Stocks/Data/NASDAQ Historical.RDATA")

```

```{r Data Update}
Ticker_Pull_Function(Location = "C://Users//aayorde//desktop//")
```

```{r Pool Reduction}
Start = Sys.time()
PR_Stage = PR_Appendage(Combined_Results,parallel = T)
print(Sys.time() - Start)

storage = list()
p = progress_estimated(nrow(PR_Stage))
for(i in 1:nrow(PR_Stage)){
  Ticker = PR_Stage$Stock[i]
  TMP = try(FinViz_Metric_Pull(Ticker),
            silent = T)
  storage[[i]] = TMP
  p$pause(0.5)$tick()$print()
}
storage2 = storage[sapply(storage,class) %in% "data.frame"]
Metrics = plyr::ldply(storage2,data.frame)
```


# Verifying Data Quality

```{r}
load(file = "//climsidfs07/RefEng/1 Ref. Engineering (SH, Scroll & IPD)/13) Analytics/Small Projects/Stocks/Data/NASDAQ Historical.RDATA")

glimpse(Combined_Results)

Stock_summary = psych::describeBy(as.matrix(Combined_Results[,1:6]),
                                  group = Combined_Results$Stock,
                                  na.rm = T,
                                  interp = T,
                                  skew = T,
                                  ranges = T,
                                  trim = 0.025,
                                  quant = c(0.05,0.25,0.75,0.95),
                                  IQR = T,
                                  omit = T)

```



```{r}
load(file = "C://Users//aayorde//desktop//NASDAQ Historical.RDATA")
Train_Set = Training_Set_Function(Combined_Results)

load(file = "C://Users//aayorde//desktop//Window_Results.RDATA")
Train_Set = Window_Results
Windows = Train_Set %>%
  group_by(Stock) %>%
  select(contains("Window")) %>%
  summarise_all(mean)
write.csv(Windows,
          file = "C://users//aayorde//desktop//Optimized_Windows.csv")

Train_Red = na.omit(Train_Set)
save(Train_Red,
     file = "C://Users//aayorde//Desktop//Stock_Train.Rdata")


## Feature Selection
Stocks = unique(Window_Results$Stock)
p = progress_estimated(n = length(Stocks),min_time = 3)
Names = list()
for(i in 1:length(Stocks)){
  p$pause(0.1)$tick()$print()
  TMP = Train_Set %>%
    filter(Stock == Stocks[i])
  Output = try(Stepwise_Log(TMP))
  if("try-error" %in% class(Output)){
    Names[[i]] = Output
  }else{
    Cols = c(Stocks[i],Output$Names)
    Names[[i]] = Cols
  }
}

Final_Names = Names[sapply(Names,class) == "character"]
TMP_Pull = function(x){Stock = x[1]}
TMP_Cols = function(x){Columns = x[-c(1,2)]}
Stocks = sapply(Final_Names,TMP_Pull)
Columns = sapply(Final_Names,TMP_Cols)
Output = list(Stocks = Stocks,Columns = Columns)

save(Output,
     file = "C://users//aayorde//desktop//Final_Names.RDATA")
```


## Generated Records
*Description of any generated data rows with the reasoning behind the creation.*

## Merged Data
*Execution and justification for various joins and aggregations used to create a central data set.*

## Reformatted Data
*Execution and justification for different modifications to the data (syntactic or order based changes)*

# Modeling {.tabset}

## Modeling Technique
*Document the actual modeling technique that is to be used.*

## Modeling Assumptions
*Record any assumptions that the specific model technique takes, uniform distribution, no missing values, etc.*

## Test Design
*Describe the plan for defining training, testing, and evaluation of the modeling technique. This is normally centered around defining how to best segment the data.*

## Parameter Settings 
*List the parameters and their chosen values, along with the rationale for the choice of parameter settings.*

## Models
*These are the actual models produced by the modeling tool, not a report.*

## Model Description
*Report on the interpretation of the models and document any difficulties encountered with their meanings.*

## Model Assessment
*Rank models in terms of the evaluation criteria. Note that this only evaluates models, not the entire scope of the project. Should be in terms similar to accuracy and/or generality.*

## Revised Parameter Settings
*Discuss methodology for the tuning of the hyper parameters and how the "best" final model was chosen.*


# Evaluation {.tabset}

## Assessment of Data Mining Results
*Summary of results in relation to business objective along with a statement of if the original objectives have been met.*

## Approved Models
*Formal listing of "final" models to be used to address the business problem at hand.*

## Review of Process
*Summarize the process review and highlight activities that have been missed and those that should be repeated.*

## List of Possible Actions
*List the potential further actions, along with the reasons for and against each option.*

## Decision
*Describe the decision as to how to proceed, along with the rationale.*


# Deployment {.tabset}


## Deployment Plan
*Summarize the deployment strategy, including the necessary steps and how to perform them.*

## Monitoring and Maintenance Plan
*Summarize the monitoring and maintenance strategy, including the necessary steps and how to perform them*

## Experience Documentation
*Summarize important experience, pitfalls, hints for future projects that may be similar in some way.*

