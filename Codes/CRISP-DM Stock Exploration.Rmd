---
output: html_document
editor_options: 
  chunk_output_type: console
---
---
title: "Stock Strategy Exploration and Automation"
author: 
- Abram Yorde
- Karen Richard
- Paul Fullenkamp
date: "October 1, 2018"
output: 
  html_document:
    theme: cerulean
    toc: true
    toc_depth: 1
    toc_float: true
---

```{r setup, include=FALSE}
######################## Functions ########################
is_installed = function(mypkg)
  is.element(mypkg, installed.packages()[, 1])
load_or_install <- function(package_names)
{
  for (package_name in package_names)
  {
    if (!is_installed(package_name))
    {
      install.packages(package_name, dependencies = TRUE)
    }
    library(
      package_name,
      character.only = TRUE,
      quietly = TRUE,
      verbose = FALSE
    )
  }
}
sourceDir <- function(path, trace = TRUE, ...) {
    for (nm in list.files(path, pattern = "\\.[RrSsQq]$")) {
       if(trace) cat(nm,":")           
       source(file.path(path, nm), ...)
       if(trace) cat("\n")
    }
}
##########################################################################
Required_Packages = c('tidyverse','installr','psych','quantmod','lubridate','dygraphs','doParallel','XML')
load_or_install(Required_Packages)

## Checking for R updates
updateR(fast = F,
        browse_news = F,
        install_R = T,
        copy_packages = F,
        copy_Rprofile.site = T,
        keep_old_packages = F,
        update_packages = F,
        start_new_R = F,
        quit_R = T,
        print_R_versions = T,
        GUI = F,
        to_checkMD5sums = T,
        keep_install_file = F)

Root_Folder = getwd()

## Loading Required Functions
sourceDir(paste0(getwd(),'/Codes/Functions/'))
```

## Initial Raw Data Pull

```{r Initial Data Pull}
# NASDAQ_Stocks = read.csv(paste0(Root_Folder,"/Data/NASDAQ.csv"))
# AMEX_Stocks = read.csv(paste0(Root_Folder,"/Data/AMEX.csv"))
# NYSE_Stocks = read.csv(paste0(Root_Folder,"/Data/NYSE.csv"))
# 
# Total_Stocks = bind_rows(NASDAQ_Stocks)
# Dump = list()
# 
# p = progress_estimated(n = nrow(Total_Stocks),min_time = 3)
# for(i in 1:nrow(Total_Stocks)){
#   p$pause(0.1)$tick()$print()
#   ticker = as.character(Total_Stocks$Symbol[i])
#   
# 
#   stockData = try(getSymbols(
#     ticker,
#     src = "yahoo",
#     auto.assign = FALSE) %>% 
#       as.data.frame() %>%
#       mutate(Date = ymd(rownames(.)))) 
#   if("try-error" %in% class(stockData)){
#     Dump[[i]] = stockData
#   }else{
#     colnames(stockData) = c("Open","High","Low","Close","Volume","Adjusted","Date")
#     stockData$Stock = ticker
#     Dump[[i]] = stockData
#   }
# }
# list.condition <- sapply(Dump, function(x) class(x) == "data.frame")
# output.list  <- Dump[list.condition]
# Combined_Results = plyr::ldply(output.list,data.frame)

# NA_Sum = function(x){Count = sum(is.na(x))}
# Check = Combined_Results %>%
#   group_by(Stock) %>%
#   summarise_all(NA_Sum)

# save(Combined_Results,
#      file = "//climsidfs07/RefEng/1 Ref. Engineering (SH, Scroll & IPD)/13) Analytics/Small Projects/Stocks/Data/NASDAQ Historical.RDATA")

```

```{r Data Update}
Ticker_Pull_Function(Location = "C://Users//aayorde//desktop//")
```

```{r Pool Reduction}
Start = Sys.time()
print("Initial Stat Calculation for Pool Selection")
PR_Stage = PR_Appendage(Combined_Results,parallel = T)
Sys.time() - Start

## Initial Pool Reduction
NASDAQ_Stocks = read.csv(paste0(Root_Folder,"/Data/NASDAQ.csv"))
PR_Red_1 = left_join(PR_Stage,NASDAQ_Stocks,by = c("Stock" = "Symbol")) %>%
  select(-c(X,Summary.Quote,LastSale)) %>%
  group_by(Sector) %>%
  filter(Price_Growth >= 15,
         Opt >= 0,
         Ratio >= 1,
         Volume_Trajectory >= 0) %>%
  mutate(Reward_Mean_Score = dense_rank(desc(Reward_Mean)),
         Risk_Mean_Score = dense_rank(desc(Risk_Mean)),
         Price_Growth_Score = dense_rank(desc(Price_Growth)),
         Volume_Norm_Score = dense_rank(desc(Volume_Norm))) %>%
  filter(Reward_Mean_Score <= quantile(Reward_Mean_Score,0.10,na.rm = T) |
           Risk_Mean_Score <= quantile(Risk_Mean_Score,0.10,na.rm = T) |
           Price_Growth_Score <= quantile(Price_Growth_Score,0.10,na.rm = T) |
           Volume_Norm_Score <= quantile(Volume_Norm_Score,0.10,na.rm = T)) %>%
  na.omit()

## Appending FinViz Stats
storage = list()
print("Beginning Pool Selection FinViz Stat Pull")
p = progress_estimated(nrow(PR_Red_1))
for(i in 1:nrow(PR_Red_1)){
  Ticker = PR_Red_1$Stock[i]
  TMP = try(FinViz_Metric_Pull(Ticker),
            silent = T)
  storage[[i]] = TMP
  p$pause(0.5)$tick()$print()
}
Metrics = plyr::ldply(storage[sapply(storage,class) %in% "data.frame"],data.frame)

Pool_Results = PR_Red_1 %>%
  left_join(Metrics) %>%
  mutate(Profit.Margin = as.numeric(str_remove(Profit.Margin,"%"))/100,
         Oper..Margin = as.numeric(str_remove(Oper..Margin,"%"))/100,
         Change = as.numeric(str_remove(Change,"%"))/100,
         Price = as.numeric(as.character(Price)),
         Prev.Close = as.numeric(as.character(Prev.Close)),
         ATR = as.numeric(as.character(ATR)),
         Beta = as.numeric(as.character(Beta)),
         Perf.YTD = as.numeric(str_remove(Perf.YTD,"%"))/100,
         Perf.Year = as.numeric(str_remove(Perf.Year,"%"))/100,
         Perf.Half.Y = as.numeric(str_remove(Perf.Half.Y,"%"))/100,
         Perf.Quarter = as.numeric(str_remove(Perf.Quarter,"%"))/100,
         Perf.Month = as.numeric(str_remove(Perf.Month,"%"))/100,
         Perf.Week = as.numeric(str_remove(Perf.Week,"%"))/100,
         RSI..14. = as.numeric(as.character(RSI..14.)),
         EPS..ttm. = as.numeric(str_remove(EPS..ttm.,"%"))/100,
         EPS.next.Y = as.numeric(str_remove(EPS.next.Y,"%"))/100,
         EPS.next.Q = as.numeric(str_remove(EPS.next.Q,"%"))/100,
         EPS.this.Y = as.numeric(str_remove(EPS.this.Y,"%"))/100,
         EPS.next.Y.1 = as.numeric(str_remove(EPS.next.Y.1,"%"))/100,
         EPS.next.5Y = as.numeric(str_remove(EPS.next.5Y,"%"))/100,
         EPS.past.5Y = as.numeric(str_remove(EPS.past.5Y,"%"))/100,
         Sales.past.5Y = as.numeric(str_remove(Sales.past.5Y,"%"))/100,
         Sales.Q.Q = as.numeric(str_remove(Sales.Q.Q,"%"))/100,
         EPS.Q.Q = as.numeric(str_remove(EPS.Q.Q,"%"))/100,
         ROA = as.numeric(str_remove(ROA,"%"))/100,
         ROE = as.numeric(str_remove(ROE,"%"))/100,
         ROI = as.numeric(str_remove(ROI,"%"))/100,
         X52W.High = as.numeric(str_remove(X52W.High,"%"))/100,
         X52W.Low = as.numeric(str_remove(X52W.Low,"%"))/100,
         Target.Price = as.numeric(as.character(Target.Price)),
         Short.Ratio = as.numeric(as.character(Short.Ratio)),
         Short.Float = as.numeric(str_remove(Short.Float,"%"))/100,
         Payout = as.numeric(str_remove(Payout,"%"))/100,
         Gross.Margin = as.numeric(str_remove(Gross.Margin,"%"))/100,
         Inst.Trans = as.numeric(str_remove(Inst.Trans,"%"))/100,
         Inst.Own = as.numeric(str_remove(Inst.Own,"%"))/100,
         Insider.Trans = as.numeric(str_remove(Insider.Trans,"%"))/100,
         Insider.Own = as.numeric(str_remove(Insider.Own,"%"))/100,
         X52W.Low = as.numeric(str_remove(X52W.Low,"%"))/100,
         LT.Debt.Eq = as.numeric(as.character(LT.Debt.Eq)),
         Debt.Eq = as.numeric(as.character(Debt.Eq)),
         Current.Ratio = as.numeric(as.character(Current.Ratio)),
         Quick.Ratio = as.numeric(as.character(Quick.Ratio)),
         P.FCF = as.numeric(as.character(P.FCF)),
         P.C = as.numeric(as.character(P.C)),
         P.B = as.numeric(as.character(P.B)),
         P.S = as.numeric(as.character(P.S)),
         PEG = as.numeric(as.character(PEG)),
         Forward.P.E = as.numeric(as.character(Forward.P.E)),
         P.E = as.numeric(as.character(P.E)),
         Recom = as.numeric(as.character(Recom)),
         Employees = as.numeric(as.character(Employees)),
         Target.Price = as.numeric(as.character(Target.Price)),
         Target.Price = as.numeric(as.character(Target.Price)),
         Target.Price = as.numeric(as.character(Target.Price))
         ) %>%
  select(-c(Volume,Avg.Volume,Rel.Volume,SMA200,SMA50,SMA20)) %>%
  filter(Profit.Margin >= 0,
         Oper..Margin >= 0,
         EPS.next.Y >= 0,
         ROA >= 0,
         ROE >= 0,
         ROI >= 0)

## Reducing Historical Data to Reduced Selections
Reduced_Results = Combined_Results %>%
  filter(Stock %in% Pool_Results$Stock)

## Saving Pool Results and Reduced Raw Data
save(Pool_Results,Reduced_Results,
     file = "C://users//aayorde//desktop//Pool_Results.RDATA")
```

## Training Set Creation

```{r Training Set Creation}
## Loading Pool Results
load(file = "C://users//aayorde//desktop//Pool_Results.RDATA")

## Building Optimized Training Set
Train_Set = Training_Set_Function(Reduced_Results)
Train_Set = na.omit(Train_Set)

## Tabulating Optimized Window Sizes
Windows = Train_Set %>%
  group_by(Stock) %>%
  select(contains("Window")) %>%
  summarise_all(mean)

## Feature Selection
Stocks = unique(Train_Set$Stock)
p = progress_estimated(n = length(Stocks),min_time = 3)
Names = list()
Remove = c("Date","Stock","Adj_Smooth","Max","Days","PR","PR_1D")
for(i in 1:length(Stocks)){
  p$pause(0.1)$tick()$print()
  TMP = Train_Set %>%
    filter(Stock == Stocks[i])
  Output = try(Variable_Importance_Reduction(TMP,
                                             Target = "Buy",
                                             Remove = Remove))
  if("try-error" %in% class(Output)){
    Names[[i]] = Output
  }else{
    Cols = c(Stocks[i],Output)
    Names[[i]] = Cols
  }
}

Final_Names = Names[sapply(Names,class) == "character"]
TMP_Pull = function(x){Stock = x[1]}
TMP_Cols = function(x){Columns = x[-c(1,2)]}
Stocks = sapply(Final_Names,TMP_Pull)
Columns = sapply(Final_Names,TMP_Cols)
Important_Features = list(Stocks = Stocks,Columns = Columns)

save(Important_Features,Train_Set,Windows,
     file = "C://users//aayorde//desktop//Training_Prep.RDATA")
```


## Generated Records
*Description of any generated data rows with the reasoning behind the creation.*

## Merged Data
*Execution and justification for various joins and aggregations used to create a central data set.*

## Reformatted Data
*Execution and justification for different modifications to the data (syntactic or order based changes)*

# Modeling {.tabset}

## Modeling Technique
*Document the actual modeling technique that is to be used.*

## Modeling Assumptions
*Record any assumptions that the specific model technique takes, uniform distribution, no missing values, etc.*

## Test Design
*Describe the plan for defining training, testing, and evaluation of the modeling technique. This is normally centered around defining how to best segment the data.*

## Parameter Settings 
*List the parameters and their chosen values, along with the rationale for the choice of parameter settings.*

## Models
*These are the actual models produced by the modeling tool, not a report.*

## Model Description
*Report on the interpretation of the models and document any difficulties encountered with their meanings.*

## Model Assessment
*Rank models in terms of the evaluation criteria. Note that this only evaluates models, not the entire scope of the project. Should be in terms similar to accuracy and/or generality.*

## Revised Parameter Settings
*Discuss methodology for the tuning of the hyper parameters and how the "best" final model was chosen.*


# Evaluation {.tabset}

## Assessment of Data Mining Results
*Summary of results in relation to business objective along with a statement of if the original objectives have been met.*

## Approved Models
*Formal listing of "final" models to be used to address the business problem at hand.*

## Review of Process
*Summarize the process review and highlight activities that have been missed and those that should be repeated.*

## List of Possible Actions
*List the potential further actions, along with the reasons for and against each option.*

## Decision
*Describe the decision as to how to proceed, along with the rationale.*


# Deployment {.tabset}


## Deployment Plan
*Summarize the deployment strategy, including the necessary steps and how to perform them.*

## Monitoring and Maintenance Plan
*Summarize the monitoring and maintenance strategy, including the necessary steps and how to perform them*

## Experience Documentation
*Summarize important experience, pitfalls, hints for future projects that may be similar in some way.*

